---
title: "reproducibility - simulation"
output: rmarkdown::html_vignette
editor_options:
  chunk_output_type: console
vignette: >
  %\VignetteIndexEntry{simulation}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

This script requires that the working directory includes the folders "data", "results", and "manuscript". We obtained our results using R 4.2.2 (2022-10-31) with cornet 0.0.7 (2023-XX-XX) on a local machine (aarch64-apple-darwin20, macOS Ventura 13.1). Floating point errors might lead to slightly different results on other platforms.

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo=TRUE,eval=FALSE)
setwd("~/Desktop/cornet")
#utils::install.packages(pkgs=c("randomForest","e1071"))
#devtools::install_github("rauschenberger/cornet")
```

# Graphical abstract

```{r abstract}
grDevices::pdf("manuscript/figure_idea.pdf",width=5,height=2.5)

box <- function(x,y,width=0.22,height=0.2,labels="",cex=1,col="black",...){
  xs <- x + 0.5*c(-1,-1,1,1)*width
  ys <- y + 0.5*c(-1,1,1,-1)*height
  graphics::polygon(x=xs,y=ys,border=col,lwd=2,...)
  graphics::text(x=x,y=y,labels=labels,col=col,cex=cex)
}

graphics::par(mar=c(0,0,0,0))
graphics::plot.new()
graphics::plot.window(xlim=c(0,1),ylim=c(0,1))

v <- h <- 0.1

box(x=0+h,y=0.5,labels="outcomes,\nfeatures")
#graphics::text(x=h,y=0.495,labels="+")
box(x=0.5,y=1-v,labels="initial binary\nclassification",col="red")
box(x=0.5,y=0+v,labels="numerical\nprediction",col="blue")
box(x=1-h,y=0.5,labels="final binary\nclassification",col="red")

d <- 0.02
graphics::arrows(x0=0.2+d,y0=0.5+c(-d,d),x1=0.4-d,y1=c(v,1-v),lwd=2,col=c("blue","red"))
graphics::arrows(x0=0.6+d,y0=c(v,1-v),x1=0.8-d,y1=0.5+c(-d,d),lwd=2,col=c("blue","red"))

#graphics::text(x=0.32,y=0.6,labels=expression(gamma),col=red,cex=2)
graphics::text(x=0.4,y=0.55,labels="binary outcome:\nlogistic regression",col="red",cex=0.7,pos=3)
#graphics::text(x=0.32,y=0.4,labels=expression(beta),col=blue,cex=2)
graphics::text(x=0.4,y=0.45,labels="numerical outcome:\nlinear regression",col="blue",cex=0.7,pos=1)
#graphics::text(x=0.95,y=0.5,labels=bquote(sigma~pi),col="grey",cex=2)
graphics::text(x=0.63,y=0.5,labels="combine\npredicted\nprobabilities",col="grey",cex=0.7)
graphics::text(x=0.8,y=0.3,labels="transform\npredicted values to\npredicted probabilities",col="grey",cex=0.7,pos=1)

grDevices::dev.off()
```


# Analysis

```{r analysis}
iter <- 1000
set.seed(1)
frame <- data.frame(cor=runif(n=iter,min=0,max=0.9),
                    n=round(runif(n=iter,min=100,max=200)),
                    prob=runif(n=iter,min=0.01,max=0.1),
                    sd=runif(n=iter,min=1,max=2),
                    exp=runif(n=iter,min=0.1,max=2),
                    frac=runif(n=iter,min=0.5,max=0.9))

ridge <- lasso <- list()
pb <- utils::txtProgressBar(min=0,max=nrow(frame),width=20,style=3)
for(i in seq_len(nrow(frame))){
    utils::setTxtProgressBar(pb=pb,value=i)
    set.seed(i)
    data <- do.call(what=cornet:::.simulate,args=cbind(frame[i,],p=500))
    set.seed(i)
    ridge[[i]] <- do.call(what=cornet:::cv.cornet,args=c(data,alpha=0,rf=TRUE,svm=TRUE))
    set.seed(i)
    lasso[[i]] <- do.call(what=cornet:::cv.cornet,args=c(data,alpha=1,rf=TRUE,svm=TRUE))
}
names(lasso) <- names(ridge) <- paste0("set",seq_len(nrow(frame)))
save(lasso,ridge,frame,file="results/simulation.RData")

writeLines(text=capture.output(utils::sessionInfo(),cat("\n"),
        sessioninfo::session_info()),con="results/info_sim.txt")
```

# Figures

```{r figure_BOX}
#--- boxplot of different metrics ---
load("results/simulation.RData",verbose=TRUE)

fuse0 <- fuse1 <- list()
for(i in c("deviance","class","mse","mae","auc")){
  fuse0[[i]] <- sapply(ridge,function(x) (x[[i]]["combined"]-x[[i]]["binomial"]))
  fuse1[[i]] <- sapply(lasso,function(x) (x[[i]]["combined"]-x[[i]]["binomial"]))
  #if(i=="auc"){fuse0[[i]] <- -fuse0[[i]]; fuse1[[i]] <- -fuse1[[i]]}
}

grDevices::pdf("manuscript/figure_BOX.pdf",width=6,height=4)
graphics::par(mar=c(1.9,1.9,0.1,0.1))
graphics::plot.new()
ylim <- range(unlist(fuse0),unlist(fuse1))
at <- seq(from=1,to=9,by=2)
graphics::plot.window(xlim=c(min(at)-0.6,max(at)+0.6),ylim=ylim)
graphics::axis(side=2)
graphics::abline(h=0,col="grey",lty=2)
graphics::abline(v=at+1,col="grey",lty=2)
graphics::box()
graphics::boxplot(fuse1,at=at-0.5,add=TRUE,axes=FALSE,col="white",border="black")
graphics::boxplot(fuse0,at=at+0.5,add=TRUE,axes=FALSE,col="white",border="darkgrey")
labels <- names(fuse1)
labels <- ifelse(labels=="class","mcr",labels)
labels <- ifelse(labels %in% c("mcr","mse","mae","auc"),toupper(labels),labels)
for(i in seq_along(labels)){
  graphics::axis(side=1,at=at[i],labels=bquote(Delta ~ .(labels[i])))
}
grDevices::dev.off()

# decrease
sapply(fuse1,function(x) mean(x<0)) # lasso
sapply(fuse0,function(x) mean(x<0)) # ridge

# constant
sapply(fuse1,function(x) mean(x==0)) # lasso
sapply(fuse0,function(x) mean(x==0)) # ridge

# increase
sapply(fuse1,function(x) mean(x>0)) # lasso
sapply(fuse0,function(x) mean(x>0)) # ridge
```

```{r figure_TAB}
#--- plot of percentage changes ---
load("results/simulation.RData",verbose=TRUE)

loss <- list()
loss$ridge <- as.data.frame(t(sapply(ridge,function(x) x$deviance)))
loss$lasso <- as.data.frame(t(sapply(lasso,function(x) x$deviance)))

data <- list()
for(i in c("ridge","lasso")){
  data[[i]] <- data.frame(row.names=rownames(frame))
  data[[i]]$"(1)" <- 100*(loss[[i]]$binomial-loss[[i]]$intercept)/loss[[i]]$intercept
  data[[i]]$"(2)" <- 100*(loss[[i]]$combined-loss[[i]]$intercept)/loss[[i]]$intercept
  data[[i]]$"(3)" <- 100*(loss[[i]]$combined-loss[[i]]$binomial)/loss[[i]]$binomial
}

row <- colnames(data$lasso)
col <- colnames(frame)
txt <- expression(rho,n,s,sigma,t,q)

for(k in c("ridge","lasso")){
  grDevices::pdf(paste0("manuscript/figure_",k,".pdf"),width=6.5,height=4)
  graphics::par(mfrow=c(length(row),length(col)),
              mar=c(0.2,0.2,0.2,0.2),oma=c(4,4,0,0))
  for(i in seq_along(row)){
    for(j in seq_along(col)){
      y <- data[[k]][[row[i]]]
      x <- frame[[col[j]]]
      graphics::plot.new()
      graphics::plot.window(xlim=range(x),ylim=range(y),xaxs="i")
      graphics::box()
      graphics::abline(h=0,lty=1,col="grey")
      graphics::points(y=y,x=x,cex=0.5,pch=16,col=ifelse(y>0,"black","grey"))
      line <- stats::loess.smooth(y=y,x=x,evaluation=200)
      graphics::lines(x=line$x,y=line$y,col="black",lty=2,lwd=1)
      if(j==1){
        graphics::mtext(text=row[i],side=2,line=2.5,las=2)
        graphics::axis(side=2)
      }
      if(i==length(row)){
        graphics::mtext(text=txt[j],side=1,line=2.5)
        graphics::axis(side=1)
      }
    }
  }
  grDevices::dev.off()
}

cbind(col,as.character(txt)) # verify
```

```{r examples,eval=FALSE,echo=FALSE}
# This chunk shows examples where combined regression outperforms transformed linear regression.

# Note: 'latent: binary variable contains all information', 'asymmetric + outliers: linear regression fails to capture all signal'

#rm(list=ls())
n <- 100

mode <- c("standard","latent","asymmetric","outliers")
loss <- sapply(mode,function(x) list())
for(i in seq_along(mode)){
  cat("mode:",mode[i],"\n")
  for(j in seq_len(10)){ # increase to 100
    cat("iteration:",j,"\n")
    n0 <- 100; n1 <- 10000; p <- 500
    # simulate correlated features?
    X <- matrix(data=stats::rnorm(n*p),nrow=n,ncol=p)
    beta <- stats::rbinom(n=p,size=1,prob=0.05)*stats::rnorm(n=p)
    # but then use constant signs for betas!
    eta <- X %*% beta + stats::rnorm(n=n)
    if(mode[i]=="standard"){
      y <- eta 
    } else if(mode[i]=="latent"){
      z <- 2*round(1/(1+exp(-eta)))-1
      y <- 2*z + stats::rnorm(n)
    } else if(mode[i]=="asymmetric"){
      y <- ifelse(eta<0,-sqrt(abs(eta)),eta^2)
    } else if(mode[i]=="outliers"){
      y <- eta + stats::rbinom(n=n,size=1,prob=0.05)*5*sd(eta) # allow for sign switch
    }
    foldid <- rep(c(0,1),times=c(n0,n1))
    loss[[mode[i]]][[j]] <- cv.cornet(y=y,cutoff=0,X=X,rf=TRUE,svm=TRUE) # use hold-out method instead (unlimited testing data are available!)
  }
}


graphics::par(mfrow=c(2,2),mar=c(3,3,1,1))
pos <- c(binomial=1,combined=2,gaussian=3)
col <- c(binomial="red",combined="grey",gaussian="blue")
cex <- 0.7
names <- c("binomial","combined","gaussian")
for(i in seq_along(mode)){
  frame <- as.data.frame(t(sapply(loss[[i]],function(x) x$deviance)))
  frame <- frame/frame$intercept # optional standardisation
  #graphics::plot.new()
  #graphics::plot.window(xlim=c(min(pos)-0.5,max(pos)+0.5),ylim=range(frame))
  #graphics::box()
  #graphics::axis(side=1,at=pos,labels=names(pos),cex.axis=cex)
  #graphics::axis(side=2,cex.axis=cex)
  graphics::boxplot(x=frame[,names],at=pos[names],col=col[names],cex.axis=0.7,main=i)
  for(j in c("binomial","combined","gaussian")){
    #graphics::boxplot(x=frame[[j]],at=pos[[j]],col=col[[j]],add=TRUE,ann=FALSE)
    mean <- mean(frame[[j]])
    graphics::points(x=pos[j],y=mean,pch=16)
    if(j=="combined"){next}
    pvalue <- stats::wilcox.test(x=frame$combined,y=frame[[j]],alternative="less")$p.value
    graphics::text(x=mean(c(pos["combined"],pos[[j]])),y=min(frame),labels=paste0("p=",signif(pvalue,digits=2)),pos=3,cex=0.7)
  }
}



deviance <- lapply(loss,function(x) as.data.frame(t(sapply(x,function(x) x$deviance))))
lapply(deviance,function(x) round(colMeans(x),digits=2))

# make boxplots
# make hypothesis tests
# repeat analysis for ridge and lasso

#--- separate chunks (to be removed) ---

# latent variable

loss <- list()
for(i in 1:10){
  n <- 100; p <- 500
  X <- matrix(stats::rnorm(n*p),nrow=n,ncol=p)
  eta <- X[,1]+X[,2]+X[,3]+stats::rnorm(n)
  z <- 2*round(1/(1+exp(-eta)))-1
  y <- 2*z + stats::rnorm(n)
  loss[[i]] <- cv.cornet(y=y,cutoff=0,X=X)
}
dev <- as.data.frame(t(sapply(loss,function(x) x$dev)))
colMeans(dev)
stats::wilcox.test(x=dev$combined,y=dev$gaussian,paired=TRUE,alternative="less")

# asymmetric

loss <- list()
for(i in 1:10){
  n <- 100; p <- 500
  X <- matrix(stats::rnorm(n*p),nrow=n,ncol=p)
  eta <- X[,1]+X[,2]+X[,3]+stats::rnorm(n)
  y <- ifelse(eta<0,-sqrt(abs(eta)),eta^2)
  loss[[i]] <- cv.cornet(y=y,cutoff=0,X=X)
}
dev <- as.data.frame(t(sapply(loss,function(x) x$dev)))
colMeans(dev)
stats::wilcox.test(x=dev$combined,y=dev$gaussian,paired=TRUE,alternative="less")

# outliers

loss <- list()
for(i in 1:10){
  n <- 100; p <- 500
  X <- matrix(stats::rnorm(n*p),nrow=n,ncol=p)
  eta <- X[,1]+X[,2]+X[,3]+stats::rnorm(n)
  y <- eta + stats::rbinom(n=n,size=1,prob=0.1)*runif(n=n,min=-20,max=20)
  loss[[i]] <- cv.cornet(y=y,cutoff=0,X=X)
}
dev <- as.data.frame(t(sapply(loss,function(x) x$dev)))
colMeans(dev)
stats::wilcox.test(x=dev$combined,y=dev$gaussian,paired=TRUE,alternative="less")

```


```{r development,eval=FALSE,echo=FALSE}
#--- adding noise ---

# example where combined regression outperforms logistic regression:

loss <- list()
for(i in 1:10){
  n <- 100; p <- 500
  X <- matrix(stats::rnorm(n*p),nrow=n,ncol=p)
  eta <- X[,1]+X[,2]+X[,3]+stats::rnorm(n)
  y <- eta
  loss[[i]] <- cv.cornet(y=y,cutoff=0,X=X)
}
dev <- as.data.frame(t(sapply(loss,function(x) x$dev)))
colMeans(dev)
stats::wilcox.test(x=dev$combined,y=dev$binomial,paired=TRUE,alternative="less")


#boxplot(y~z)
dev <- t(sapply(loss,function(x) x$deviance))
dev <- dev/dev[,"intercept"]
graphics::par(mar=c(3,3,1,1))
graphics::plot(dev[,"gaussian"],ylim=range(dev),type="l",col="blue")
graphics::lines(dev[,"binomial"],ylim=range(dev),type="l",col="red")
graphics::lines(dev[,"combined"],ylim=range(dev),type="l",col="black")
rowMeans(apply(dev,1,rank))

# combined example (useless)

w <- 0.9
loss <- list()
for(i in 1:10){
  n <- 100; p <- 500
  X <- matrix(stats::rnorm(n*p),nrow=n,ncol=p)
  eta <- scale(X[,1]+X[,2]+X[,3])
  eta[is.na(eta)] <- 0
  z <- 2*round(1/(1+exp(-eta)))-1
  y <- (1-w)*eta + w*z + 0.5*stats::rnorm(n) 
  loss[[i]] <- cv.cornet(y=y,cutoff=0,X=X)
}
dev <- as.data.frame(t(sapply(loss,function(x) x$dev)))
colMeans(dev)
stats::wilcox.test(x=dev$combined,y=dev$gaussian,paired=TRUE,alternative="less")
stats::wilcox.test(x=dev$combined,y=dev$binomial,paired=TRUE,alternative="less")
graphics::boxplot(y~z)
table(y>=0,z)


#--- non-linear effects ---

auc <- list()
for(i in 1:10){
set.seed(i)
n <- 100; p <- 500
X <- matrix(stats::rnorm(n*p),nrow=n,ncol=p)
beta <- stats::rbinom(n=p,size=1,prob=0.05)*stats::rnorm(n=p)
XX <- sign(X)*abs(X)^rep(sample(c(1/3,3),size=p,replace=TRUE),each=n)
y <- XX %*% beta
auc[[i]] <- cv.cornet(y=y,cutoff=0,X=X,alpha=1)$auc
}
rowMeans(sapply(auc,function(x) rank(x)))
rowMeans(sapply(auc,function(x) x))

#i <- sample(seq_len(p),size=1)
#plot(X[,i],XX[,i])
#y <- round(1/(1 + exp(- X %*% beta)))
#y <- (2*y-1) + ifelse(y==0,rnorm(n,sd=0.1),abs(rnorm(n,sd=2)))

#plot(y); abline(h=0,lty=2)

#y <- round(1/(1 + exp(- X %*% beta)))

load("results/simulation_old.RData",verbose=TRUE)
loss <- t(sapply(lasso,function(x) x$auc))
prop <- loss[,"binomial"]/loss[,"gaussian"]
j <- which.max(prop)
frame[j,]
loss[j,]

auc <- list()
for(i in 1:10){
  set.seed(j)
  data <- .simulate(n=frame$n[j],p=500,cor=frame$cor[j],prob=frame$prob[j],sd=frame$sd[j],exp=frame$exp[j],frac=frame$frac[j])
  auc[[i]] <- cv.cornet(y=data$y,cutoff=data$cutoff,X=data$X,alpha=1)$auc
}
colMeans(do.call(what="rbind",args=auc))


load("results/application.RData",verbose=TRUE)
test <- lapply(loss,function(x) sapply(x,function(x) x$auc))
prop <- lapply(test,function(x) x["combined",]/x["gaussian",])
mean(unlist(prop))


#--- another trial ---
data <- .simulate(n=150,p=500,cor=0.5,prob=0.05,sd=1,exp=0.5,frac=0.1)
auc <- cv.cornet(y=data$y,cutoff=data$cutoff,X=data$X,alpha=1)$auc


#--- performance of RF and SVM ---
load("results/simulation.RData",verbose=TRUE)
loss <- as.data.frame(t(sapply(lasso,function(x) x$auc)))
colMeans(loss)

load("results/simulation_old.RData",verbose=TRUE)

```


